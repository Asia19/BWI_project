{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\persi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import psycopg2\n",
    "import math\n",
    "import numpy as np\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.another import get_ngrams, key_dependent_dict, condition\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host=\"localhost\", database=\"postgres\", user=\"postgres\", password=\"igsvemina1201\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size - 2183895       \n",
      "number of ngrams in train corpus:\n",
      "ungirams - 829250940\n",
      "bigrams - 798949912\n",
      "trigrams - 768648884\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT count(*) from unigram_counter')\n",
    "vocab_size = cur.fetchone()[0]\n",
    "cur.execute('SELECT sum(unigram_count) from unigram_counter')\n",
    "n_unigrams = cur.fetchone()[0]\n",
    "cur.execute('SELECT sum(bigram_count) from bigram_counter')\n",
    "n_bigrams = cur.fetchone()[0]\n",
    "cur.execute('SELECT sum(trigram_count) from trigram_counter')\n",
    "n_trigrams = cur.fetchone()[0]\n",
    "\n",
    "print(f'vocab size - {vocab_size} \\\n",
    "      \\nnumber of ngrams in train corpus:\\nungirams - {n_unigrams}\\nbigrams - {n_bigrams}\\ntrigrams - {n_trigrams}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path='../data/test_v2.txt'\n",
    "test_unigram_counter = Counter()\n",
    "test_bigram_counter = Counter()\n",
    "test_trigram_counter = Counter()\n",
    "with io.open(test_file_path, encoding='utf-8', mode='rt') as test_file:\n",
    "    for line in test_file:\n",
    "        comma_pos = line.find(',')\n",
    "        line = line[comma_pos+1:].rstrip()\n",
    "        line = line[1:-1] # skip \"\n",
    "        sentence = [\"<s>\"] + line.lower().split(' ') + [\"</s>\"]\n",
    "        test_unigram_counter.update(sentence)\n",
    "        test_bigram_counter.update(get_ngrams(sentence, 2))\n",
    "        test_trigram_counter.update(get_ngrams(sentence, 3))\n",
    "with open('../data/test_unigram_counter.csv', 'w', encoding='utf-8') as unigram_counter_csv, \\\n",
    "    open('../data/test_bigram_counter.csv', 'w', encoding='utf-8') as bigram_counter_csv, \\\n",
    "    open('../data/test_trigram_counter.csv', 'w', encoding='utf-8') as trigram_counter_csv:\n",
    "    unigram_counter_csv.write('first_gram,unigram_count\\n')\n",
    "    for unigram, count in test_unigram_counter.items():\n",
    "        unigram_counter_csv.write('{} {}\\n'.format(unigram, count))\n",
    "    bigram_counter_csv.write('first_gram,second_gram,bigram_count\\n')\n",
    "    for bigram, count in test_bigram_counter.items():\n",
    "        bigram_counter_csv.write('{} {}\\n'.format(' '.join(bigram), count))\n",
    "    trigram_counter_csv.write('first_gram,second_gram,third_gram,trigram_count\\n')\n",
    "    for trigram, count in test_trigram_counter.items():\n",
    "        trigram_counter_csv.write('{} {}\\n'.format(' '.join(trigram), count))\n",
    "cur = conn.cursor()\n",
    "cur.execute('DROP TABLE IF EXISTS test_unigram_counter')\n",
    "cur.execute('DROP TABLE IF EXISTS test_bigram_counter')\n",
    "cur.execute('DROP TABLE IF EXISTS test_trigram_counter')\n",
    "cur.execute('''CREATE TABLE test_unigram_counter( first_gram VARCHAR, unigram_count INT)''')\n",
    "cur.execute('''CREATE TABLE test_bigram_counter( first_gram VARCHAR, second_gram VARCHAR, bigram_count INT)''')\n",
    "cur.execute('''CREATE TABLE test_trigram_counter( first_gram VARCHAR, second_gram VARCHAR, \\\n",
    "            third_gram VARCHAR, trigram_count INT)''')\n",
    "cur.execute(\"COPY test_unigram_counter FROM 'C:/HSE/3_module/BWI_project/data/test_unigram_counter.csv' DELIMITER ' ' CSV HEADER\")\n",
    "cur.execute(\"COPY test_bigram_counter FROM 'C:/HSE/3_module/BWI_project/data/test_bigram_counter.csv' DELIMITER ' ' CSV HEADER\")\n",
    "cur.execute(\"COPY test_trigram_counter FROM 'C:/HSE/3_module/BWI_project/data/test_trigram_counter.csv' DELIMITER ' ' CSV HEADER\")\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "# extract test unigram count from train table\n",
    "cur.execute(\"SELECT * FROM unigram_counter as T WHERE T.first_gram in (SELECT first_gram FROM test_unigram_counter)\")\n",
    "train_unigram_counter = dict(cur.fetchall())\n",
    "cur.execute(\"SELECT * FROM bigram_counter as T WHERE (T.first_gram,T.second_gram) in (SELECT first_gram,second_gram FROM test_bigram_counter)\")\n",
    "train_bigram_counter = dict(map(lambda x: ((x[0],x[1]),x[2]), cur.fetchall()))\n",
    "cur.execute(\"SELECT * FROM trigram_counter as T WHERE (T.first_gram,T.second_gram,T.third_gram) in (SELECT first_gram,second_gram,third_gram FROM test_trigram_counter)\")\n",
    "train_trigram_counter = dict(map(lambda x: (x[:-1],x[-1]), cur.fetchall()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_line(line):\n",
    "    comma_pos = line.find(',')\n",
    "    sentence = line[comma_pos+1:].rstrip()\n",
    "    sentence = sentence[1:-1] # skip \"\n",
    "    return line[:comma_pos+1], sentence\n",
    "\n",
    "def decode_sentence(sentence):\n",
    "    return '\"' + sentence + '\"'+'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new Lm\n",
    "class InterpolationLanguageModel:\n",
    "    def __init__(self, vocab_size, n=2, lambdas=[0.5,0.5]):\n",
    "        assert len(lambdas) == n\n",
    "        assert sum(lambdas) == 1.0\n",
    "        assert n==2 or n==3\n",
    "        self.n = n\n",
    "        self.lambdas = {(i+1):l for i, l in enumerate(lambdas)}        \n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def fit(self, train_counters, n_ngrams):  \n",
    "        ngrams_statistics = {1:{'total':n_ngrams[1], 'counter':train_counters[1]}, \\\n",
    "                                  2:{'total':n_ngrams[2], 'counter':train_counters[2]}, \\\n",
    "                                  3:{'total':n_ngrams[3], 'counter':train_counters[3]}}\n",
    "        # ADD-ONE SMOOTHING\n",
    "        v = self.vocab_size\n",
    "        for n in range(1,4):\n",
    "            if n == 1:\n",
    "                # probabilities for unseen ngrams\n",
    "                N = ngrams_statistics[n]['total']\n",
    "                compute_p_unseen = lambda x : (ngrams_statistics[n]['counter'].get(x,0) + 1) / (N + v)\n",
    "                ngrams_statistics[n]['probs'] = key_dependent_dict(compute_p_unseen)\n",
    "                # probabilities for known ngrams\n",
    "                for word, count in ngrams_statistics[n]['counter'].items():\n",
    "                    ngrams_statistics[n]['probs'][word] = (count + 1) / (N + v)                               \n",
    "            else:\n",
    "                ngrams_statistics[n]['probs'] = dict()\n",
    "                # probabilities for known ngrams\n",
    "                for ngram, count in ngrams_statistics[n]['counter'].items():\n",
    "                    context, word = condition(ngram), ngram[-1]\n",
    "                    condition_count = ngrams_statistics[n-1]['counter'][context]\n",
    "                    if context not in ngrams_statistics[n]['probs']:\n",
    "                        ngrams_statistics[n]['probs'][context] = dict()\n",
    "                    ngrams_statistics[n]['probs'][context][word] = (count + 1) / (condition_count + v)           \n",
    "        self.ngrams_statistics = ngrams_statistics\n",
    "    \n",
    "    def prob(self, ngram):\n",
    "        if isinstance(ngram,str):\n",
    "            return self.ngrams_statistics[1]['probs'][ngram]\n",
    "        n = len(ngram)\n",
    "        context, word = condition(ngram), ngram[-1]\n",
    "        # check if ngram is unseen\n",
    "        if context not in self.ngrams_statistics[n]['probs']:\n",
    "            self.ngrams_statistics[n]['probs'][context] = dict()\n",
    "        if word not in self.ngrams_statistics[n]['probs'][context]:\n",
    "            self.ngrams_statistics[n]['probs'][context][word] = (self.ngrams_statistics[n]['counter'].get(ngram,0) + 1) / \\\n",
    "                          (self.ngrams_statistics[n-1]['counter'].get(context,0) + self.vocab_size)\n",
    "        return self.ngrams_statistics[n]['probs'][context][word]\n",
    "            \n",
    "    def get_prob(self, ngram):\n",
    "        p = 0        \n",
    "        for n in range(self.n,0,-1):            \n",
    "            p += self.lambdas[n] * self.prob(ngram)\n",
    "            ngram = ngram[-1] if n==2 else ngram[1:]     \n",
    "        return p\n",
    "            \n",
    "    def test(self, ngram_counter):\n",
    "        assert len(list(ngram_counter.keys())[0]) == self.n\n",
    "        perplexity = 0             \n",
    "        n_ngrams = sum(ngram_counter.values())\n",
    "        for ngram, count in ngram_counter.items():\n",
    "            perplexity += count * math.log2(self.get_prob(ngram))\n",
    "        perplexity = math.pow(2, -1*(perplexity/n_ngrams))        \n",
    "        return perplexity\n",
    "    \n",
    "    def insert_missing_word(self, sentence):\n",
    "        tokenized_sentence = [\"<s>\"] + sentence.lower().split() + [\"</s>\"]\n",
    "        # slice ngrams under consideration\n",
    "        slicer = slice(1,-1) if self.n == 2 else slice(0,-2)\n",
    "        sentence_ngrams = list(get_ngrams(tokenized_sentence, self.n))[slicer]\n",
    "        spaces_inds = [m.start() for m in re.finditer(' ', sentence)]\n",
    "        minP_ngram_arg = np.argmin([self.get_prob(ngram) for ngram in sentence_ngrams])\n",
    "        minP_ngram = sentence_ngrams[minP_ngram_arg]\n",
    "        space_ind = spaces_inds[minP_ngram_arg]\n",
    "        context, word = condition(minP_ngram), minP_ngram[-1]\n",
    "        candidates = self.ngrams_statistics[self.n]['probs'][context]\n",
    "        candtidates_probs = {c:prob*self.get_prob((c,word)) for c, prob in candidates.items()}\n",
    "        missing_word = max(candtidates_probs, key=candtidates_probs.get)\n",
    "        outline = sentence[:space_ind+1] + missing_word + sentence[space_ind:]\n",
    "        return outline\n",
    "        \n",
    "    def fill_gaps(self, test_file_path, submission_file_path='submission.txt'):\n",
    "        with io.open(test_file_path, encoding='utf-8', mode='rt') as infile, \\\n",
    "            io.open(submission_file_path, encoding='utf-8', mode='wt') as outfile:\n",
    "            outfile.write(infile.readline()+'\\n')\n",
    "            for i, line in enumerate(infile):\n",
    "#                 if (i+1) % 1000 == 0:\n",
    "                print(f'working on {i+1} file', end='\\r')\n",
    "                n_line, sentence = encode_line(line)\n",
    "                sentence = self.insert_missing_word(sentence)\n",
    "                outline = n_line + decode_sentence(sentence)\n",
    "                outfile.write(outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counters = {1:train_unigram_counter, 2:train_bigram_counter, 3:train_trigram_counter}\n",
    "n_ngrams = {1:n_unigrams, 2:n_bigrams, 3:n_trigrams}\n",
    "lm = InterpolationLanguageModel(vocab_size=vocab_size)\n",
    "lm.fit(train_counters, n_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity = 1038.58\n"
     ]
    }
   ],
   "source": [
    "print(f'perplexity = {lm.test(ngram_counter=test_bigram_counter):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 1369 file\r"
     ]
    }
   ],
   "source": [
    "lm.fill_gaps(test_file_path=test_file_path, submission_file_path='../data/submission.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "uni_coc = Counter(sorted(train_unigram_counter.values()))\n",
    "log_r = np.log10(list(uni_coc.keys())).reshape(-1, 1)\n",
    "log_Z = np.log10(list(uni_coc.values())).reshape(-1, 1)\n",
    "reg = LinearRegression().fit(log_r, log_Z)\n",
    "a, b = reg.intercept_, reg.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.55806461]), array([-0.57176488]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds the slope for the best fit line\n",
    "def findBestFitSlope(x,y):\n",
    "    m = (( mean(x)*mean(y) - mean(x*y) ) / \n",
    "          ( mean(x)** 2 - mean(x**2)))\n",
    "\n",
    "    return m\n",
    "      \n",
    "#finds the intercept for the best fit line\n",
    "def findBestFitIntercept(x,y,m):\n",
    "    c = mean(y) - m*mean(x)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_r = np.log10(r).reshape(-1, 1)\n",
    "log_Z = np.log10(Z).reshape(-1, 1)\n",
    "reg = LinearRegression().fit(log_r, log_Z)\n",
    "a, b = reg.intercept_, reg.coef_[0]\n",
    "S = np.power(10, a+b*np.log(np.arange(1, np.max(r)+2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
