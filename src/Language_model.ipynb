{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\persi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import psycopg2\n",
    "import math\n",
    "import numpy as np\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.another import get_ngrams, key_dependent_dict, condition\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host=\"localhost\", database=\"postgres\", user=\"postgres\", password=\"igsvemina1201\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size - 2183895       \n",
      "number of ngrams in train corpus:\n",
      "ungirams - 829250940\n",
      "bigrams - 798949912\n",
      "trigrams - 768648884\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT count(*) from unigram_counter')\n",
    "vocab_size = cur.fetchone()[0]\n",
    "cur.execute('SELECT sum(unigram_count) from unigram_counter')\n",
    "n_unigrams = cur.fetchone()[0]\n",
    "cur.execute('SELECT sum(bigram_count) from bigram_counter')\n",
    "n_bigrams = cur.fetchone()[0]\n",
    "cur.execute('SELECT sum(trigram_count) from trigram_counter')\n",
    "n_trigrams = cur.fetchone()[0]\n",
    "\n",
    "print(f'vocab size - {vocab_size} \\\n",
    "      \\nnumber of ngrams in train corpus:\\nungirams - {n_unigrams}\\nbigrams - {n_bigrams}\\ntrigrams - {n_trigrams}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path='../data/test_v2.txt'\n",
    "test_unigram_counter = Counter()\n",
    "test_bigram_counter = Counter()\n",
    "test_trigram_counter = Counter()\n",
    "with io.open(test_file_path, encoding='utf-8', mode='rt') as test_file:\n",
    "    for line in test_file:\n",
    "        comma_pos = line.find(',')\n",
    "        line = line[comma_pos+1:].rstrip()\n",
    "        line = line[1:-1] # skip \"\n",
    "        sentence = [\"<s>\"] + line.lower().split(' ') + [\"</s>\"]\n",
    "        test_unigram_counter.update(sentence)\n",
    "        test_bigram_counter.update(get_ngrams(sentence, 2))\n",
    "        test_trigram_counter.update(get_ngrams(sentence, 3))\n",
    "with open('../data/test_unigram_counter.csv', 'w', encoding='utf-8') as unigram_counter_csv, \\\n",
    "    open('../data/test_bigram_counter.csv', 'w', encoding='utf-8') as bigram_counter_csv, \\\n",
    "    open('../data/test_trigram_counter.csv', 'w', encoding='utf-8') as trigram_counter_csv:\n",
    "    unigram_counter_csv.write('first_gram,unigram_count\\n')\n",
    "    for unigram, count in test_unigram_counter.items():\n",
    "        unigram_counter_csv.write('{} {}\\n'.format(unigram, count))\n",
    "    bigram_counter_csv.write('first_gram,second_gram,bigram_count\\n')\n",
    "    for bigram, count in test_bigram_counter.items():\n",
    "        bigram_counter_csv.write('{} {}\\n'.format(' '.join(bigram), count))\n",
    "    trigram_counter_csv.write('first_gram,second_gram,third_gram,trigram_count\\n')\n",
    "    for trigram, count in test_trigram_counter.items():\n",
    "        trigram_counter_csv.write('{} {}\\n'.format(' '.join(trigram), count))\n",
    "cur = conn.cursor()\n",
    "cur.execute('DROP TABLE IF EXISTS test_unigram_counter')\n",
    "cur.execute('DROP TABLE IF EXISTS test_bigram_counter')\n",
    "cur.execute('DROP TABLE IF EXISTS test_trigram_counter')\n",
    "cur.execute('''CREATE TABLE test_unigram_counter( first_gram VARCHAR, unigram_count INT)''')\n",
    "cur.execute('''CREATE TABLE test_bigram_counter( first_gram VARCHAR, second_gram VARCHAR, bigram_count INT)''')\n",
    "cur.execute('''CREATE TABLE test_trigram_counter( first_gram VARCHAR, second_gram VARCHAR, \\\n",
    "            third_gram VARCHAR, trigram_count INT)''')\n",
    "cur.execute(\"COPY test_unigram_counter FROM 'C:/HSE/3_module/BWI_project/data/test_unigram_counter.csv' DELIMITER ' ' CSV HEADER\")\n",
    "cur.execute(\"COPY test_bigram_counter FROM 'C:/HSE/3_module/BWI_project/data/test_bigram_counter.csv' DELIMITER ' ' CSV HEADER\")\n",
    "cur.execute(\"COPY test_trigram_counter FROM 'C:/HSE/3_module/BWI_project/data/test_trigram_counter.csv' DELIMITER ' ' CSV HEADER\")\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "# extract test unigram count from train table\n",
    "cur.execute(\"SELECT * FROM unigram_counter as T WHERE T.first_gram in (SELECT first_gram FROM test_unigram_counter)\")\n",
    "train_unigram_counter = dict(cur.fetchall())\n",
    "cur.execute(\"SELECT * FROM bigram_counter as T WHERE (T.first_gram,T.second_gram) in (SELECT first_gram,second_gram FROM test_bigram_counter)\")\n",
    "train_bigram_counter = dict(map(lambda x: ((x[0],x[1]),x[2]), cur.fetchall()))\n",
    "cur.execute(\"SELECT * FROM trigram_counter as T WHERE (T.first_gram,T.second_gram,T.third_gram) in (SELECT first_gram,second_gram,third_gram FROM test_trigram_counter)\")\n",
    "train_trigram_counter = dict(map(lambda x: (x[:-1],x[-1]), cur.fetchall()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_line(line):\n",
    "    comma_pos = line.find(',')\n",
    "    sentence = line[comma_pos+1:].rstrip()\n",
    "    sentence = sentence[1:-1] # skip \"\n",
    "    return line[:comma_pos+1], sentence\n",
    "\n",
    "def decode_sentence(sentence):\n",
    "    return '\"' + sentence + '\"'+'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new Lm\n",
    "class InterpolationLanguageModel:\n",
    "    def __init__(self, n=2, lambdas=[0.5,0.5], smoothing='add_one', vocab_size=False):\n",
    "        assert len(lambdas) == n\n",
    "        assert sum(lambdas) == 1.0\n",
    "        assert n==2 or n==3\n",
    "        self.n = n\n",
    "        self.lambdas = {(i+1):l for i, l in enumerate(lambdas)}\n",
    "        self.addone_smoothing = smoothing == 'add_one'\n",
    "        self.good_tuning = smoothing == 'good_turing'\n",
    "        if self.addone_smoothing and not vocab_size:\n",
    "            raise Exception\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def fit(self, train_counters, n_ngrams):  \n",
    "        ngrams_statistics = {1:{'total':n_ngrams[1], 'counter':train_counters[1]}, \\\n",
    "                                  2:{'total':n_ngrams[2], 'counter':train_counters[2]}, \\\n",
    "                                  3:{'total':n_ngrams[3], 'counter':train_counters[3]}}\n",
    "        # ADD-ONE SMOOTHING\n",
    "        if self.addone_smoothing:\n",
    "            one, v = 1, self.vocab_size\n",
    "            for n in range(1,4):\n",
    "                # probabilities for unseen ngrams\n",
    "                if n == 1:\n",
    "                    compute_p_unseen = lambda x : (ngrams_statistics[n]['counter'].get(x,0) + one) / \\\n",
    "                                  (ngrams_statistics[n]['total'] + v)\n",
    "                else:\n",
    "                    compute_p_unseen = lambda x : (ngrams_statistics[n]['counter'].get(x,0) + one) / \\\n",
    "                                  (ngrams_statistics[n-1]['counter'].get(condition(x),0) + v)\n",
    "                ngrams_statistics[n]['probs'] = key_dependent_dict(compute_p_unseen)\n",
    "                # probabilities for known ngrams\n",
    "                for ngram, count in ngrams_statistics[n]['counter'].items():\n",
    "                    condition_count = ngrams_statistics[n]['total'] if n==1 else \\\n",
    "                                      ngrams_statistics[n-1]['counter'][condition(ngram)]\n",
    "                    ngrams_statistics[n]['probs'][ngram] = (count + one) / (condition_count + v)           \n",
    "        self.ngrams_statistics = ngrams_statistics\n",
    "            \n",
    "    def get_prob(self, ngram):\n",
    "        p = 0\n",
    "        for n in range(self.n,0,-1):\n",
    "            p += self.lambdas[n] * self.ngrams_statistics[n]['probs'][ngram]\n",
    "            ngram = ngram[-1] if n==2 else ngram[1:]     \n",
    "        return p\n",
    "            \n",
    "    def test(self, ngram_counter):\n",
    "        assert len(list(ngram_counter.keys())[0]) == self.n\n",
    "        perplexity = 0             \n",
    "        n_ngrams = sum(ngram_counter.values())\n",
    "        for ngram, count in ngram_counter.items():\n",
    "            perplexity += count * math.log(self.get_prob(ngram))\n",
    "        perplexity = math.pow(2, -1*(perplexity/n_ngrams))        \n",
    "        return perplexity\n",
    "    \n",
    "    def insert_missing_word(self, sentence):\n",
    "        tokenized_sentence = [\"<s>\"] + sentence.lower().split() + [\"</s>\"]\n",
    "        # slice ngrams under consideration\n",
    "        slicer = slice(1,-1) if self.n == 2 else slice(0,-2)\n",
    "        sentence_ngrams = list(get_ngrams(tokenized_sentence, self.n))[slicer]\n",
    "        minP_ngram_arg = np.argmin([self.get_prob(ngram) for ngram in sentence_ngrams])\n",
    "        minP_ngram = sentence_ngrams[minP_ngram_arg]\n",
    "        spaces_inds = [m.start() for m in re.finditer(' ', sentence)]\n",
    "        space_ind = spaces_inds[minP_ngram_arg]\n",
    "        context, word = minP_ngram[:-1], minP_ngram[-1]\n",
    "        candidates = [key[-1] for key in self.ngrams_statistics[self.n]['probs'].keys() if key[:-1] == context]\n",
    "        argmax = np.argmax([self.get_prob(context+(candidate,))*self.get_prob((candidate,word)) for candidate in candidates])\n",
    "        missing_word = candidates[argmax]\n",
    "        outline = sentence[:space_ind+1] + missing_word + sentence[space_ind:]\n",
    "        return outline\n",
    "        \n",
    "    def fill_gaps(self, test_file_path, submission_file_path='submission.txt'):\n",
    "        with io.open(test_file_path, encoding='utf-8', mode='rt') as infile, \\\n",
    "            io.open(submission_file_path, encoding='utf-8', mode='wt') as outfile:\n",
    "            outfile.write(infile.readline()+'\\n')\n",
    "            for i, line in enumerate(infile):\n",
    "#                 if (i+1) % 1000 == 0:\n",
    "                print(f'working on {i+1} file', end='\\r')\n",
    "                n_line, sentence = encode_line(line)\n",
    "                sentence = self.insert_missing_word(sentence)\n",
    "                outline = n_line + decode_sentence(sentence)\n",
    "                outfile.write(outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counters = {1:train_unigram_counter, 2:train_bigram_counter, 3:train_trigram_counter}\n",
    "n_ngrams = {1:n_unigrams, 2:n_bigrams, 3:n_trigrams}\n",
    "lm = InterpolationLanguageModel(smoothing='add_one',vocab_size=vocab_size)\n",
    "lm.fit(train_counters, n_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity = 121.23\n",
      "working on 170 file\r"
     ]
    }
   ],
   "source": [
    "print(f'perplexity = {lm.test(ngram_counter=test_bigram_counter):.2f}')\n",
    "lm.fill_gaps(test_file_path=test_file_path, submission_file_path='../data/submission.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "uni_coc = Counter(sorted(train_unigram_counter.values()))\n",
    "log_r = np.log10(list(uni_coc.keys())).reshape(-1, 1)\n",
    "log_Z = np.log10(list(uni_coc.values())).reshape(-1, 1)\n",
    "reg = LinearRegression().fit(log_r, log_Z)\n",
    "a, b = reg.intercept_, reg.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.55806461]), array([-0.57176488]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds the slope for the best fit line\n",
    "def findBestFitSlope(x,y):\n",
    "    m = (( mean(x)*mean(y) - mean(x*y) ) / \n",
    "          ( mean(x)** 2 - mean(x**2)))\n",
    "\n",
    "    return m\n",
    "      \n",
    "#finds the intercept for the best fit line\n",
    "def findBestFitIntercept(x,y,m):\n",
    "    c = mean(y) - m*mean(x)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_r = np.log10(r).reshape(-1, 1)\n",
    "log_Z = np.log10(Z).reshape(-1, 1)\n",
    "reg = LinearRegression().fit(log_r, log_Z)\n",
    "a, b = reg.intercept_, reg.coef_[0]\n",
    "S = np.power(10, a+b*np.log(np.arange(1, np.max(r)+2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new Lm\n",
    "class InterpolationLanguageModel:\n",
    "    def __init__(self, lambda1=0.1, lambda2=0.3, lambda3=0.6, smoothing='add_one', vocab_size=False):\n",
    "        self.lambdas = {1:lambda1, 2:lambda2, 3:lambda3}\n",
    "        self.addone_smoothing = smoothing == 'add_one'\n",
    "        self.good_tuning = smoothing == 'good_turing'\n",
    "        if self.addone_smoothing and not vocab_size:\n",
    "            raise Exception\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def fit(self, train_counters, n_ngrams):  \n",
    "        ngrams_statistics = {1:{'total':n_ngrams[1], 'counter':train_counters[1]}, \\\n",
    "                                  2:{'total':n_ngrams[2], 'counter':train_counters[2]}, \\\n",
    "                                  3:{'total':n_ngrams[3], 'counter':train_counters[3]}}\n",
    "        # ADD-ONE SMOOTHING\n",
    "        if self.addone_smoothing:\n",
    "            one, v = 1, self.vocab_size\n",
    "            for n in range(1,4):\n",
    "                # probabilities for unseen ngrams\n",
    "                if n == 1:\n",
    "                    compute_p_unseen = lambda x : (ngrams_statistics[n]['counter'].get(x,0) + one) / \\\n",
    "                                  (ngrams_statistics[n]['total'] + v)\n",
    "                else:\n",
    "                    compute_p_unseen = lambda x : (ngrams_statistics[n]['counter'].get(x,0) + one) / \\\n",
    "                                  (ngrams_statistics[n-1]['counter'].get(condition(x),0) + v)\n",
    "                ngrams_statistics[n]['probs'] = key_dependent_dict(compute_p_unseen)\n",
    "                # probabilities for known ngrams\n",
    "                for ngram, count in ngrams_statistics[n]['counter'].items():\n",
    "                    condition_count = ngrams_statistics[n]['total'] if n==1 else \\\n",
    "                                      ngrams_statistics[n-1]['counter'][condition(ngram)]\n",
    "                    ngrams_statistics[n]['probs'][ngram] = (count + one) / (condition_count + v)\n",
    "        # GOOD TURING\n",
    "        else:\n",
    "            for n in ngrams_statistics:\n",
    "                ngrams_statistics[n]['counter_of_counts'] = Counter(sorted(ngrams_statistics[n]['counter'].values()))\n",
    "                ngrams_statistics[n]['smoothed_counter'] = defaultdict(int)\n",
    "                # probabilities for unseen ngrams\n",
    "                ngrams_statistics[n]['probs'] = defaultdict(lambda: ngrams_statistics[n]['counter_of_counts'] / \\\n",
    "                                                                       ngrams_statistics[n]['total'])\n",
    "            def adjust_count(count, n):\n",
    "                return (count + 1) * ngrams_statistics[n]['counter_of_counts'][count+1] / \\\n",
    "                        ngrams_statistics[n]['counter_of_counts'][count]\n",
    "            # adjuct counts(compute c*) and compute probabilities for known ngrams \n",
    "            for n in range(1,4):\n",
    "                for ngram, count in ngrams_statistics[n]['counter'].items():\n",
    "                    new_c = adjust_count(count, n)\n",
    "                    if ngram == 'confirmed':\n",
    "                        print(ngrams_statistics[n]['counter_of_counts'][count+1], ngrams_statistics[n]['counter_of_counts'][count])\n",
    "                    ngrams_statistics[n]['smoothed_counter'][ngram] = new_c\n",
    "                    denominator = ngrams_statistics[n]['total'] if n==1 else \\\n",
    "                                  ngrams_statistics[n-1]['smoothed_counter'][condition(ngram)]\n",
    "                    if denominator==0: \n",
    "                        print('n',n,'\\nngram',ngram,'\\ncondition',condition(ngram),'\\n')\n",
    "                    ngrams_statistics[n]['probs'][ngram] = new_c / denominator\n",
    "        self.ngrams_statistics = ngrams_statistics\n",
    "            \n",
    "            \n",
    "    def logprob(self, trigram, alpha1=1, alpha2=1):\n",
    "        p = 0\n",
    "        p = self.lambdas[1] * self.ngrams_statistics[1]['probs'][trigram[-1]]\n",
    "        p += self.lambdas[2] * self.ngrams_statistics[2]['probs'][trigram[1:]]\n",
    "        p += self.lambdas[3] * self.ngrams_statistics[3]['probs'][trigram]       \n",
    "        return math.log2(p)\n",
    "            \n",
    "    def test(self):\n",
    "        perplexity = 0             \n",
    "        n_trigrams = sum(test_trigram_counter.values())\n",
    "        for trigram, count in test_trigram_counter.items():\n",
    "            perplexity += count * self.logprob(trigram)\n",
    "        perplexity = math.pow(2, -1*(perplexity/n_trigrams))     \n",
    "        \n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOD TURING\n",
    "        else:\n",
    "            for n in ngrams_statistics:\n",
    "                ngrams_statistics[n]['counter_of_counts'] = Counter(ngrams_statistics[n]['counter'].values())\n",
    "                ngrams_statistics[n]['smoothed_counter'] = defaultdict(int)\n",
    "                # probabilities for unseen ngrams\n",
    "                ngrams_statistics[n]['probs'] = defaultdict(lambda: ngrams_statistics[n]['counter_of_counts'] / \\\n",
    "                                                                       ngrams_statistics[n]['total'])\n",
    "            def adjust_count(count, n):\n",
    "                return (count + 1) * ngrams_statistics[n]['counter_of_counts'][count+1] / \\\n",
    "                        ngrams_statistics[n]['counter_of_counts'][count]\n",
    "            # adjuct counts(compute c*) and compute probabilities for known ngrams \n",
    "            for n in range(1,4):\n",
    "                for ngram, count in ngrams_statistics[n]['counter'].items():\n",
    "                    new_c = adjust_count(count, n)\n",
    "                    if ngram == 'confirmed':\n",
    "                        print(ngrams_statistics[n]['counter_of_counts'][count+1], ngrams_statistics[n]['counter_of_counts'][count])\n",
    "                    ngrams_statistics[n]['smoothed_counter'][ngram] = new_c\n",
    "                    denominator = ngrams_statistics[n]['total'] if n==1 else \\\n",
    "                                  ngrams_statistics[n-1]['smoothed_counter'][condition(ngram)]\n",
    "                    if denominator==0: \n",
    "                        print('n',n,'\\nngram',ngram,'\\ncondition',condition(ngram),'\\n')\n",
    "                    ngrams_statistics[n]['probs'][ngram] = new_c / denominator\n",
    "        self.ngrams_statistics = ngrams_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# something about backoff smoothing\n",
    "            # N_c - the count of things we've seen c times\n",
    "#         else:\n",
    "#             if trigram in train_trigram_counter:\n",
    "#                 p = self.lambda1 * self.unigram_probs[trigram[-1]]\n",
    "#                 p += self.lambda2 * self.bigram_probs[trigram[1:]]\n",
    "#                 p += self.lambda3 * self.trigram_probs[trigram]\n",
    "#             elif trigram[1:] in train_bigram_counter:\n",
    "#                 p = (self.lambda1 * alpha1 + self.lambda2) * self.bigram_probs[trigram[1:]]\n",
    "#                 p += self.lambda3 * self.trigram_probs[trigram]\n",
    "#             elif trigram[-1] in train_unigram_counter:\n",
    "#                 p = (self.lambda1 * alpha2 + self.lambda2 * 0.4 + self.lambda3) * self.trigram_probs[trigram]\n",
    "#             else:\n",
    "#                 return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first version of fit\n",
    "    def fit(self, train_counters, n_unigrams):        \n",
    "        if self.addone_smoothing:\n",
    "            one, v = 1, self.vocab_size\n",
    "            self.unigram_probs = defaultdict(lambda: one / (n_unigrams + v))\n",
    "            self.bigram_probs = dict()\n",
    "            self.trigram_probs = dict()\n",
    "            for unigram in train_unigram_counter:\n",
    "                self.unigram_probs[unigram] = (train_unigram_counter[unigram] + one) / (n_unigrams + v)\n",
    "            for bigram in train_bigram_counter:\n",
    "                condition, _ = bigram\n",
    "                self.bigram_probs[bigram] = (train_bigram_counter[bigram] + one) / (train_unigram_counter[condition] + v)\n",
    "            for trigram in train_trigram_counter:\n",
    "                condition, _ = trigram[:-1], trigram[-1]\n",
    "                self.trigram_probs[trigram] = (train_trigram_counter[trigram] + one) / (train_bigram_counter[condition] + v)\n",
    "        elif self.good_tuning:\n",
    "            counter_of_unigram_counts = Counter(train_unigram_counter.values())\n",
    "            counter_of_bigram_counts = Counter(train_bigram_counter.values())\n",
    "            counter_of_trigram_counts = Counter(train_trigram_counter.values())\n",
    "            counters_of_counts = {1:counter_of_unigram_counts, 2:counter_of_bigram_counts, 3:counter_of_trigram_counts}\n",
    "            self.smoothed_unigram_counter = dict()\n",
    "            self.smoothed_bigram_counter = dict()\n",
    "            self.smoothed_trigram_counter = dict()\n",
    "            def adjust_count(count, n):\n",
    "                return (count + 1) * counters_of_counts[n][count+1] / counters_of_counts[n][count]\n",
    "            # GOOD TURING\n",
    "            # probabilities for unseen ngrams\n",
    "            self.unigram_probs = defaultdict(lambda: counters_of_counts[1][1] / n_unigrams)\n",
    "            self.bigram_probs = defaultdict(lambda: counters_of_counts[2][1] /  n_bigrams)\n",
    "            self.trigram_probs = defaultdict(lambda: counters_of_counts[3][1] / n_trigrams)\n",
    "            # adjuct counts(compute c*) and compute ngram probabilities\n",
    "            for unigram in train_unigram_counter:\n",
    "                self.smoothed_unigram_counter[unigram] = adjust_count(train_unigram_counter[unigram], 1)\n",
    "            for unigram, count in self.smoothed_unigram_counter.items():\n",
    "                self.unigram_probs[unigram] = count  / n_unigrams\n",
    "            for bigram in train_bigram_counter:\n",
    "                self.smoothed_bigram_counter[bigram] = adjust_count(train_bigram_counter[bigram], 2)\n",
    "            for bigram, count in self.smoothed_bigram_counter.items():\n",
    "                condition, _ = bigram\n",
    "                self.bigram_probs[bigram] = count / self.smoothed_unigram_counter[condition]\n",
    "            for trigram in train_trigram_counter:\n",
    "                self.smoothed_trigram_counter[trigram] = adjust_count(train_trigram_counter[trigram], 3)\n",
    "            for trigram, count in smoothed_trigram_counter.items():\n",
    "                condition, _ = trigram[:-1], trigram[-1]\n",
    "                self.trigram_probs[trigram] = count / self.smoothed_bigram_counter[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self, lambda1=0.1, lambda2=0.3, lambda3=0.6, addone_smoothing=True):\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.lambda3 = lambda3\n",
    "        self.addone_smoothing = addone_smoothing\n",
    "        self.good_tuning = not addone_smoothing\n",
    "    \n",
    "    def fit(self):        \n",
    "        if self.addone_smoothing:\n",
    "            one, v = 1, vocab_size\n",
    "            self.unigram_probs = defaultdict(lambda: one / (n_unigrams + v))\n",
    "            self.bigram_probs = defaultdict(lambda: one /  .......)\n",
    "            self.trigram_probs = defaultdict(lambda: one / .....)\n",
    "            for unigram in train_unigram_counter:\n",
    "                self.unigram_probs[unigram] = (train_unigram_counter[unigram] + one) / (n_unigrams + v)\n",
    "            for bigram in train_bigram_counter:\n",
    "                condition, _ = bigram\n",
    "                self.bigram_probs[bigram] = (train_bigram_counter[bigram] + one) / (train_unigram_counter[condition] + v)\n",
    "            for trigram in train_trigram_counter:\n",
    "                condition, _ = trigram[:-1], trigram[-1]\n",
    "                self.trigram_probs[trigram] = (train_trigram_counter[trigram] + one) / (train_bigram_counter[condition] + v)\n",
    "        elif self.good_tuning:\n",
    "            counter_of_unigram_counts = Counter(train_unigram_counter.values())\n",
    "            counter_of_bigram_counts = Counter(train_bigram_counter.values())\n",
    "            counter_of_trigram_counts = Counter(train_trigram_counter.values())\n",
    "            counters_of_counts = {1:counter_of_unigram_counts, 2:counter_of_bigram_counts, 3:counter_of_trigram_counts}\n",
    "            self.smoothed_unigram_counter = defaultdict(lambda:0)\n",
    "            self.smoothed_bigram_counter = defaultdict(lambda: 0)\n",
    "            self.smoothed_trigram_counter = defaultdict(lambda: 0)\n",
    "            self.unigram_probs = defaultdict(lambda: counters_of_counts[1][1] / n_unigrams)\n",
    "            self.bigram_probs = defaultdict(lambda: counters_of_counts[2][1] /  n_bigrams)\n",
    "            self.trigram_probs = defaultdict(lambda: counters_of_counts[3][1] / n_trigrams)\n",
    "            def adjust_count(count, n):\n",
    "                return (count + 1) * counters_of_counts[n][count+1] / counters_of_counts[n][count]\n",
    "            # good tuning\n",
    "            for unigram in train_unigram_counter:\n",
    "                self.smoothed_unigram_counter[unigram] = adjust_count(train_unigram_counter[unigram], 1)\n",
    "            for bigram in train_bigram_counter:\n",
    "                self.smoothed_bigram_counter[bigram] = adjust_count(train_bigram_counter[bigram], 2)\n",
    "            for trigram in train_trigram_counter:\n",
    "                self.smoothed_trigram_counter[trigram] = adjust_count(train_trigram_counter[trigram], 3)\n",
    "            for unigram, count in self.smoothed_unigram_counter.items():\n",
    "                self.unigram_probs[unigram] = count  / n_unigrams\n",
    "            for bigram, count in self.smoothed_bigram_counter.items():\n",
    "                condition, _ = bigram\n",
    "                self.bigram_probs[bigram] = count / self.smoothed_unigram_counter[condition]\n",
    "            for trigram, count in smoothed_trigram_counter.items():\n",
    "                condition, _ = trigram[:-1], trigram[-1]\n",
    "                self.trigram_probs[trigram] = count / (train_bigram_counter[condition]\n",
    "                                                       \n",
    "            \n",
    "            \n",
    "            \n",
    "    def logprob(self, trigram, alpha1=1, alpha2=1):\n",
    "        p = 0\n",
    "        if self.addone_smoothing:\n",
    "            p = self.lambda1 * self.unigram_probs[trigram[-1]]\n",
    "            p += self.lambda2 * self.bigram_probs[trigram[1:]]\n",
    "            p += self.lambda3 * self.trigram_probs[trigram]\n",
    "            \n",
    "            # N_c - the count of things we've seen c times\n",
    "#         else:\n",
    "#             if trigram in train_trigram_counter:\n",
    "#                 p = self.lambda1 * self.unigram_probs[trigram[-1]]\n",
    "#                 p += self.lambda2 * self.bigram_probs[trigram[1:]]\n",
    "#                 p += self.lambda3 * self.trigram_probs[trigram]\n",
    "#             elif trigram[1:] in train_bigram_counter:\n",
    "#                 p = (self.lambda1 * alpha1 + self.lambda2) * self.bigram_probs[trigram[1:]]\n",
    "#                 p += self.lambda3 * self.trigram_probs[trigram]\n",
    "#             elif trigram[-1] in train_unigram_counter:\n",
    "#                 p = (self.lambda1 * alpha2 + self.lambda2 * 0.4 + self.lambda3) * self.trigram_probs[trigram]\n",
    "#             else:\n",
    "#                 return         \n",
    "        return math.log(p)\n",
    "            \n",
    "    def test(self):\n",
    "        perplexity = 0             \n",
    "        n_trigrams = sum(test_trigram_counter.values())\n",
    "        for trigram, count in test_trigram_counter.items():\n",
    "            perplexity += count * self.logprob(trigram)\n",
    "        perplexity = math.pow(2, -1*(perplexity/n_trigrams))     \n",
    "        \n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.11243776960517"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LanguageModel()\n",
    "lm.fit()\n",
    "lm.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
